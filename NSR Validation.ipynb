{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "liquid-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import scipy.stats as st\n",
    "from sklearn.preprocessing import scale\n",
    "import glob, os\n",
    "from os.path import basename\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recreational-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "perct=0.81 #percentage training\n",
    "percv=0.19 #percentage validation\n",
    "exclude = set()\n",
    "#exclude.update([\"234\"])# no P annotated:\n",
    "#datfiles=glob.glob(qtdbpath+\"*.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interim-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense,Activation,Dropout\n",
    "from keras.layers import LSTM,Bidirectional, GRU #could try TimeDistributed(Dense(...))\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers,regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow.compat.v1.keras.backend as KTF\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# functions\n",
    "def get_ecg_data(datfile): \n",
    "    ## convert .dat/q1c to numpy arrays\n",
    "    recordname=os.path.basename(datfile).split(\".dat\")[0]\n",
    "    recordpath=os.path.dirname(datfile)\n",
    "    print(recordname,recordpath)\n",
    "#     cwd=os.getcwd()\n",
    "#     os.chdir(recordpath) ## somehow it only works if you chdir. \n",
    "\n",
    "    annotator='atr'\n",
    "    annotation = wfdb.rdann(recordname, extension=annotator, sampfrom=0,sampto = None)\n",
    "    \n",
    "    record = wfdb.rdrecord(recordname, sampfrom=0,sampto = None) #wfdb.showanncodes()\n",
    "    \n",
    "    Vctrecord=np.transpose(record.p_signal)\n",
    "    \n",
    "    VctAnnotationHot=np.zeros( (2,len(Vctrecord[1])), dtype=np.int)\n",
    "    VctAnnotationHot[1] = 1;\n",
    "    \n",
    "    VctAnnotations=list(zip(annotation.sample,annotation.symbol)) ## zip coordinates + annotations (N),(t) etc)\n",
    "    #print(VctAnnotations)\n",
    "    for i in range(len(VctAnnotations)):\n",
    "        if(  VctAnnotations[i][1] == 'Â·' or \n",
    "           VctAnnotations[i][1] == 'N' or \n",
    "            VctAnnotations[i][1] == 'L' or  \n",
    "            VctAnnotations[i][1] == 'R' or \n",
    "           VctAnnotations[i][1] == 'B' or \n",
    "           VctAnnotations[i][1] == 'A' or\n",
    "           VctAnnotations[i][1] == 'a' or \n",
    "           VctAnnotations[i][1] == 'J' or \n",
    "           VctAnnotations[i][1] == 'S' or \n",
    "           VctAnnotations[i][1] == 'V' or  \n",
    "           VctAnnotations[i][1] == 'r' or  \n",
    "           VctAnnotations[i][1] == 'F' or \n",
    "           VctAnnotations[i][1] == 'e' or \n",
    "           VctAnnotations[i][1] == 'j' or\n",
    "           VctAnnotations[i][1] == 'n' or \n",
    "           VctAnnotations[i][1] == 'E' or \n",
    "           VctAnnotations[i][1] == '/' or \n",
    "           VctAnnotations[i][1] == 'f' or\n",
    "           VctAnnotations[i][1] == 'Q' or  \n",
    "           VctAnnotations[i][1] == '?'):\n",
    "            VctAnnotationHot[0][VctAnnotations[i][0]] = 1;  \n",
    "            VctAnnotationHot[1][VctAnnotations[i][0]] = 0;  \n",
    "    VctAnnotationHot=np.transpose(VctAnnotationHot)\n",
    "    Vctrecord=np.transpose(Vctrecord) # transpose to (timesteps,feat)\n",
    "\n",
    "#     os.chdir(cwd)\n",
    "    return Vctrecord, VctAnnotationHot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continuous-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitseq(x,n,o):\n",
    "\tn = int(n)\n",
    "\to = int(o)\n",
    "\t#split seq; should be optimized so that remove_seq_gaps is not needed. \n",
    "\tupper= int(math.ceil( x.shape[0] / n) *n)\n",
    "\tprint(\"splitting on\",n,\"with overlap of \",o,\t\"total datapoints:\",x.shape[0],\"; upper:\",upper)\n",
    "\tfor i in range(0,upper,n):\n",
    "\t\t#print(i)\n",
    "\t\tif i==0:\n",
    "\t\t\tpadded=np.zeros( ( o+n+o,x.shape[1])   ) ## pad with 0's on init\n",
    "\t\t\tpadded[o:,:x.shape[1]] = x[i:i+n+o,:]\n",
    "\t\t\txpart=padded\n",
    "\t\telse:\n",
    "\t\t\txpart=x[i-o:i+n+o,:]\n",
    "\t\tif xpart.shape[0]<i:\n",
    "\n",
    "\t\t\tpadded=np.zeros( (o+n+o,xpart.shape[1])  ) ## pad with 0's on end of seq\n",
    "\t\t\tpadded[:xpart.shape[0],:xpart.shape[1]] = xpart\n",
    "\t\t\txpart=padded\n",
    "\n",
    "\t\txpart=np.expand_dims(xpart,0)## add one dimension; so that you get shape (samples,timesteps,features)\n",
    "\t\ttry:\n",
    "\t\t\txx=np.vstack(  (xx,xpart) )\n",
    "\t\texcept UnboundLocalError: ## on init\n",
    "\t\t\txx=xpart\n",
    "\tprint(\"output: \",xx.shape)\n",
    "\treturn(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "disciplinary-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_new(x):\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i] = scale( x[i], axis=0, with_mean=True, with_std=True, copy=True )\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "heavy-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "backed-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoaddDatFiles(datfiles):  \n",
    "    for datfile in datfiles:\n",
    "        print(datfile)\n",
    "        if basename(datfile).split(\".\",1)[0] in exclude:\n",
    "            continue\n",
    "        qf=os.path.splitext(datfile)[0]+'.atr'\n",
    "        if os.path.isfile(qf):\n",
    "#             print(\"yes\",qf,datfile)\n",
    "            x,y=get_ecg_data(datfile)\n",
    "\n",
    "            x,y=splitseq(x,1000,0),splitseq(y,1000,0) ## create equal sized numpy arrays of n size and overlap of o \n",
    "\n",
    "            x = normalize_new(x)\n",
    "            ## todo; add noise, shuffle leads etc. ?\n",
    "            try: ## concat\n",
    "                xx=np.vstack(  (xx,x) )\n",
    "                yy=np.vstack(  (yy,y) )\n",
    "            except NameError: ## if xx does not exist yet (on init)\n",
    "                xx = x\n",
    "                yy = y\n",
    "    return(xx,yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "parental-arrival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16273.dat\n",
      "16273 \n",
      "splitting on 1000 with overlap of  0 total datapoints: 11354112 ; upper: 11355000\n",
      "output:  (11355, 1000, 2)\n",
      "splitting on 1000 with overlap of  0 total datapoints: 11354112 ; upper: 11355000\n",
      "output:  (11355, 1000, 2)\n",
      "(11355, 1000, 2)\n",
      "(11355, 1000, 1)\n",
      "!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "## MITDB\n",
    "\n",
    "\n",
    "#qtdbpath_test=\"/home/federico/imec-nl/ECG-LSTM-Annotator/physionet/physiobank/database/mitdb/\"#sys.argv[1] ## first argument = qtdb database from physionet. \n",
    "\n",
    "# load data\n",
    "import glob\n",
    "datfiles_test=glob.glob(\"*.dat\")\n",
    "datfiles_test\n",
    "\n",
    "\n",
    "xxtmitdb,yytmitdb=LoaddDatFiles(datfiles_test) # training data. \n",
    "xxtmitdb,yytmitdb=unison_shuffled_copies(xxtmitdb,yytmitdb) ### shuffle\n",
    "\n",
    "\n",
    "# USE SINGLE CHANNEL\n",
    "pcha=xxtmitdb[:,:,1]; \n",
    "pchb=xxtmitdb[:,:,0];\n",
    "\n",
    "all_x_test = np.vstack([pchb]) # only channel zero\n",
    "all_x_test = np.expand_dims(all_x_test,2)\n",
    "all_lab_test = np.vstack([1-yytmitdb]) # only channel zero\n",
    "print(all_lab_test.shape)\n",
    "print(all_x_test.shape)\n",
    "print(\"!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "thorough-freight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxv/validation shape: 11355, Seqlength: 1000, Features: 1\n"
     ]
    }
   ],
   "source": [
    "seqlength=xxtmitdb.shape[1]\n",
    "features=1#xxtmitdb.shape[2]\n",
    "dimout=yytmitdb.shape[2]\n",
    "print(\"xxv/validation shape: {}, Seqlength: {}, Features: {}\".format(xxtmitdb.shape[0],seqlength,features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "attempted-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.compat.v1.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "# def f1(y_true, y_pred):\n",
    "#     y_pred = K.round(y_pred)\n",
    "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "#     fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "#     p = tp / (tp + fp + K.epsilon())\n",
    "#     r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "#     f1 = 2*p*r / (p+r+K.epsilon())\n",
    "#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "#     return K.mean(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "supposed-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras import layers\n",
    "from keras import optimizers\n",
    "# from tensorflow.python.keras.models import Sequential,load_model\n",
    "\n",
    "import numpy as np\n",
    "from keras.activations import softmax\n",
    "from keras.objectives import categorical_crossentropy\n",
    "\n",
    "def getmodel_one():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, kernel_regularizer=l2(l=0.01), input_shape=(seqlength, features)))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))#, input_shape=(seqlength, features)) ) ### bidirectional ---><---\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(l=0.01)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(dimout, activation='sigmoid'))\n",
    "    adam = tf.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    weights = np.array([64.0, 0.1,])\n",
    "    model.compile(loss=f1_loss, #loss=weighted_categorical_crossentropy(weights), \n",
    "                  optimizer=adam, \n",
    "                  metrics=['categorical_accuracy', f1_m,precision_m, recall_m, matthews_correlation]) #(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy']) \n",
    "    print(model.summary())\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "early-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session(gpu_fraction=0.8):\n",
    "\t#allocate % of gpu memory.\n",
    "\tnum_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "\tgpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "\tif num_threads:\n",
    "\t\treturn tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "\telse:\n",
    "\t\treturn tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "architectural-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE CPU otherwise need a lot of memory in the GPU\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "num_cores = 4\n",
    "GPU = 1\n",
    "CPU = 0\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 6\n",
    "if CPU:\n",
    "    num_CPU = 7\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "KTF.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "moderate-amount",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1000, 32)          64        \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 1000, 128)         49664     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000, 32)          4128      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1000, 32)          128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000, 2)           66        \n",
      "=================================================================\n",
      "Total params: 54,562\n",
      "Trainable params: 54,242\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 11355 samples\n",
      "11355/11355 [==============================] - 1873s 165ms/sample - loss: 0.7901 - categorical_accuracy: 0.8549 - f1_m: 0.8478 - precision_m: 0.8350 - recall_m: 0.8613 - matthews_correlation: 0.6902\n",
      "Train on 11355 samples\n",
      "11355/11355 [==============================] - 1923s 169ms/sample - loss: 0.4722 - categorical_accuracy: 0.9601 - f1_m: 0.9600 - precision_m: 0.9601 - recall_m: 0.9600 - matthews_correlation: 0.9200\n",
      "Train on 11355 samples\n",
      "11355/11355 [==============================] - 1971s 174ms/sample - loss: 0.3127 - categorical_accuracy: 0.9930 - f1_m: 0.9930 - precision_m: 0.9931 - recall_m: 0.9928 - matthews_correlation: 0.9859\n",
      "Train on 11355 samples\n",
      "11355/11355 [==============================] - 2225s 196ms/sample - loss: 0.2887 - categorical_accuracy: 0.9941 - f1_m: 0.9941 - precision_m: 0.9941 - recall_m: 0.9940 - matthews_correlation: 0.9881\n",
      "Train on 11355 samples\n",
      "11355/11355 [==============================] - 2269s 200ms/sample - loss: 0.2857 - categorical_accuracy: 0.9943 - f1_m: 0.9943 - precision_m: 0.9943 - recall_m: 0.9942 - matthews_correlation: 0.9885\n",
      "Train on 11355 samples\n",
      "11355/11355 [==============================] - 2298s 202ms/sample - loss: 0.2839 - categorical_accuracy: 0.9943 - f1_m: 0.9942 - precision_m: 0.9943 - recall_m: 0.9942 - matthews_correlation: 0.9885\n",
      "Train on 11355 samples\n",
      "11355/11355 [==============================] - 4707s 414ms/sample - loss: 0.2824 - categorical_accuracy: 0.9944 - f1_m: 0.9944 - precision_m: 0.9944 - recall_m: 0.9943 - matthews_correlation: 0.9888\n",
      "Train on 11355 samples\n",
      "11355/11355 [==============================] - 2089s 184ms/sample - loss: 0.2809 - categorical_accuracy: 0.9946 - f1_m: 0.9946 - precision_m: 0.9946 - recall_m: 0.9945 - matthews_correlation: 0.9891\n"
     ]
    }
   ],
   "source": [
    "# call keras/tensorflow and build lstm model \n",
    "#index_ = np.linspace(0, xxt.shape[0]-1, xxt.shape[0] ).astype(int);\n",
    "#shuffle(index_)\n",
    "\n",
    "# TRAIN\n",
    "KTF.set_session(session)\n",
    "\n",
    "## early stopping monitoring mette\n",
    "EarlyStopping(monitor='matthews_correlation', mode='max')\n",
    "\n",
    "with tf.compat.v1.Session(): #switch to /cpu:0 to use cpu \n",
    "    #if not os.path.isfile('model_mitd_binary_noover.h5'):\n",
    "    model =   getmodel_one() #build_model_gru() \n",
    "    #model = getmodel_simple();\n",
    "    for this_eps in range(8):\n",
    "        model.fit(all_x_test, all_lab_test, batch_size=100, epochs=1, verbose=1) # train the model\n",
    "        model.save('model_edb_binary_noover_eps_f1loss_2ch_'+str(this_eps)+'_.h5')\n",
    "        model.save_weights('model_weights_edb_binary_noover_eps_f1loss_2ch_'+str(this_eps)+'_.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fallen-rebound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1000, 32)          64        \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1000, 128)         49664     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000, 32)          4128      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1000, 32)          128       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000, 2)           66        \n",
      "=================================================================\n",
      "Total params: 54,562\n",
      "Trainable params: 54,242\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n",
      "114/114 [==============================] - 129s 1s/step - loss: 0.4623 - categorical_accuracy: 0.9931 - f1_m: 0.9932 - precision_m: 0.9941 - recall_m: 0.9922 - matthews_correlation: 0.9863\n",
      "f1_score: 0.993197500705719\n",
      "CatAcc: 0.9931469559669495\n",
      "Precision: 0.9941255450248718\n",
      "Recall: 0.9922717809677124\n",
      "MattCorr: 0.9864088296890259\n"
     ]
    }
   ],
   "source": [
    "#load weights because of the custom metrics \n",
    "from keras.models import load_model\n",
    "\n",
    "with tf.device('/cpu:0'): #switch to /cpu:0 to use cpu \n",
    "   \n",
    "    model = getmodel_one()\n",
    "    model.load_weights('model_weights_edb_binary_noover_eps_f1loss_2ch_1_.h5')\n",
    " \n",
    "    loss, cat_acc , f1_score, precision, recall, mettc = model.evaluate(all_x_test, all_lab_test, batch_size=100, verbose=1)\n",
    "    print('f1_score: {}'.format(f1_score))\n",
    "    print('CatAcc: {}'.format(cat_acc))\n",
    "    print('Precision: {}'.format(precision))\n",
    "    print('Recall: {}'.format(recall))\n",
    "    print('MattCorr: {}'.format(mettc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-smooth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
